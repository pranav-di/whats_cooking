{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing different Machine Learning models that solve the \"What's Cooking\" Kaggle Competition problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.kaggle.com/c/whats-cooking/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run locally, make sure the \"train.json\" and \"test.json\" files are copied into the same folder as this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "### numpy and sklearn are required for all solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "Ig59C31VWwGS",
    "outputId": "0f4ffccd-f42f-4e3e-dfd6-cef1751840b6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "!{sys.executable} -m pip install unidecode\n",
    "from unidecode import unidecode\n",
    "\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = \"english\", binary = True)\n",
    "label_encoder = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Run on Google Colab, follow these instructions:\n",
    "1) Set read_from_google_drive to True <br>\n",
    "2) Uncomment the 3 lines at the bottom of the following cell <br>\n",
    "3) Enter the folder path into the prompted area where the \"train.json\" and \"test.json\" files are housed.<br>\n",
    "   The entered path is also where solution files will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pZGCqgIKpKir",
    "outputId": "adf0c1bf-2282-49da-9221-f1c9981de713"
   },
   "outputs": [],
   "source": [
    "device_folder = \"\"\n",
    "read_from_google_drive = False\n",
    "\n",
    "# device_folder = \"gdrive/<Insert Folder Name or Path>/\"\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIAXKKa_WwGo"
   },
   "outputs": [],
   "source": [
    "def ReadDataFromDrive ():\n",
    "    training_set = json.load(open(str(device_folder) + \"train.json\"))\n",
    "    testing_set = json.load(open(str(device_folder) + \"test.json\"))\n",
    "\n",
    "    return training_set, testing_set\n",
    "\n",
    "def ReadData ():\n",
    "    \n",
    "    training_set = json.load(open(\"train.json\"))\n",
    "    testing_set = json.load(open(\"test.json\"))\n",
    "    \n",
    "    return training_set, testing_set\n",
    "\n",
    "def RemoveUnits (ingredient):\n",
    "    \n",
    "    removeList = [\"lb\", \"lbs\", \"kg\", \"g\", \"gm\", \"oz\", \"small\", \"medium\", \"large\", \"diced\", \"slice\", \"sliced\"]\n",
    "    \n",
    "    words = ingredient.split()\n",
    "    ingredient = \" \".join([word for word in words if word not in removeList])\n",
    "    \n",
    "    words = ingredient.split()\n",
    "    ingredient = \" \".join([word for word in words if word not in ENGLISH_STOP_WORDS])\n",
    "\n",
    "    return ingredient\n",
    "\n",
    "def PrepareData (X):\n",
    "    \n",
    "    X = [\" \".join([re.sub(r\"\\d+\", \"\", ingredient) for ingredient in entry.split()]) for entry in X]\n",
    "    X = [\" \".join([re.sub(r\"\\s+\", \" \", ingredient).strip() for ingredient in entry.split()]) for entry in X]\n",
    "    X = [\" \".join([str(ingredient).lower() for ingredient in entry.split()]) for entry in X]\n",
    "    X = [\" \".join([unidecode(ingredient) for ingredient in entry.split()]) for entry in X]\n",
    "\n",
    "    X = [\" \".join([ingredient.translate(str.maketrans(\"\",\"\", string.punctuation)) for ingredient in entry.split()]) for entry in X]\n",
    "    X = [\" \".join([ingredient.replace(u\"\\u2122\", \"\") for ingredient in entry.split()]) for entry in X]\n",
    "    X = [\" \".join([ingredient.replace(u\"\\u00AE\", \"\") for ingredient in entry.split()]) for entry in X]\n",
    "    X = [\" \".join([ingredient.replace(u\"\\u2019\", \"\") for ingredient in entry.split()]) for entry in X]\n",
    "\n",
    "    X = [\" \".join([RemoveUnits(ingredient) for ingredient in entry.split()]) for entry in X]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    X = [\" \".join([lemmatizer.lemmatize(ingredient) for ingredient in entry.split()]) for entry in X]\n",
    "    \n",
    "    return X\n",
    "    \n",
    "def ReadAndPreProcessData (drive = False):\n",
    "    \n",
    "    if drive == True:\n",
    "        train, test = ReadDataFromDrive()\n",
    "        \n",
    "    else:\n",
    "        train, test = ReadData() \n",
    "\n",
    "    train_ingredients = [\" \".join(entry[\"ingredients\"]) for entry in train]\n",
    "    train_cuisines = [entry[\"cuisine\"] for entry in train]\n",
    "    train_ids = [entry[\"id\"] for entry in train]\n",
    "\n",
    "    test_ingredients = [' '.join(entry[\"ingredients\"]) for entry in test]\n",
    "    test_ids = [entry[\"id\"] for entry in test]\n",
    "\n",
    "    train_ingredients = PrepareData(train_ingredients)\n",
    "    test_ingredients = PrepareData(test_ingredients)\n",
    "    \n",
    "    return train_ingredients, train_cuisines, test_ingredients, test_ids\n",
    "  \n",
    "def CreateSubmission (test_ids, y_test, stats, folder_path = device_folder):\n",
    "  \n",
    "    timestamp = str(datetime.datetime.now())[5:16]\n",
    "    timestamp = re.sub(\" \", \"_\", timestamp)\n",
    "    timestamp = re.sub(\"-\", \"_\", timestamp)\n",
    "    timestamp = re.sub(\":\", \"_\", timestamp)\n",
    "    fname = folder_path + \"submission_\" + stats + \"_\" + timestamp + \".csv\"\n",
    "    \n",
    "    solution = np.transpose(np.vstack((test_ids, y_test)))\n",
    "    solution = np.vstack((['id', 'cuisine'], solution))\n",
    "    \n",
    "    np.savetxt(fname, solution, delimiter=\",\", fmt=\"%s\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtMMba_fWwGx"
   },
   "outputs": [],
   "source": [
    "train_ingredients, train_cuisines, test_ingredients, test_ids = ReadAndPreProcessData(read_from_google_drive)\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(train_ingredients).astype(\"float16\")\n",
    "y = label_encoder.fit_transform(train_cuisines)\n",
    "\n",
    "X_test = tfidf_vectorizer.transform(test_ingredients).astype(\"float16\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2LP_jcEpKlM"
   },
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1jMKb6OmpKpB"
   },
   "source": [
    "## (1)\n",
    "## Model: Naive Bayes\n",
    "## Best Score: 70.172%\n",
    "## Requirements: sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "FfmaUGUXpKpU",
    "outputId": "33e7a5a7-2671-4372-dc84-ae393c7efdeb"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "nb = ComplementNB(alpha=0.7)\n",
    "nb.fit(X, y)\n",
    "prediction = nb.predict(X_test)\n",
    "CreateSubmission(test_ids, label_encoder.inverse_transform(prediction), \"complement_naive_bayes\", folder_path = device_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)\n",
    "## Model: k-Nearest Neighbors\n",
    "## Best Score: 75.643%\n",
    "## Requirements: sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "79bbnPSrpKo5",
    "outputId": "c5655b84-dce4-4718-8961-7e7ca676e2a1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=21)\n",
    "knn.fit(X, y)\n",
    "prediction = knn.predict(X_test)\n",
    "CreateSubmission(test_ids, label_encoder.inverse_transform(prediction), \"knn\", folder_path = device_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mI8N9t_3pKl6"
   },
   "source": [
    "## (3)\n",
    "## Model: Ensemble (Random Forests and Extra Trees)\n",
    "## Best Score: 75.693% and 77.986%\n",
    "## Requirements: sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TuXtlthrpKmD"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QmUuIhtipKmN"
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 300)\n",
    "clf.fit(X,y)\n",
    "\n",
    "predictions = label_encoder.inverse_transform(clf.predict(X_test))\n",
    "\n",
    "CreateSubmission (test_ids, predictions, \"random_forest_classifier_300_estimators\", folder_path = device_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uf9J8weEpKmc"
   },
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators = 300)\n",
    "clf.fit(X,y)\n",
    "\n",
    "predictions = label_encoder.inverse_transform(clf.predict(X_test))\n",
    "\n",
    "CreateSubmission (test_ids, predictions, \"extra_trees_classifier_300_estimators\", folder_path = device_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)\n",
    "## Model: Logistic Regression\n",
    "## Best Score: 78.439%\n",
    "## Requirements: sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoDDgyELpKlc"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bedKEM9CpKlr"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol = 1e-5, solver='sag', multi_class='multinomial')\n",
    "clf.fit(X,y)\n",
    "\n",
    "output = clf.predict_proba(X_test)\n",
    "final_output = label_encoder.inverse_transform(np.argmax(output, axis=1))\n",
    "\n",
    "CreateSubmission(test_ids, final_output, \"logistic_regression_tolerance_1e-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Y4eZqP4pKml"
   },
   "source": [
    "## (5)\n",
    "## Model: Support Vector Classifiers\n",
    "## Requirements: sklearn v0.20.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5azu8xNpKm2"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Score: 81.858%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I78A-mPDpKnK"
   },
   "outputs": [],
   "source": [
    "clf = SVC(C=10250, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape='ovr', degree=2, gamma='scale', kernel='poly',\n",
    "          max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "          tol=0.001, verbose=False)\n",
    "classifier = OneVsRestClassifier(clf)\n",
    "classifier.fit(X,y)\n",
    "\n",
    "predictions = label_encoder.inverse_transform(classifier.predict(X_test))\n",
    "CreateSubmission (test_ids, predictions, \"svc_polynomial\", folder_path = device_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HTjJjEFDpKnn"
   },
   "source": [
    "## Second Best Score: 81.476%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xv8IzLHypKn1"
   },
   "outputs": [],
   "source": [
    "clf2 = SVC(kernel='rbf', C=1, gamma=1, tol=1e-2)\n",
    "classifier2 = OneVsRestClassifier(clf2)\n",
    "classifier2.fit(X,y)\n",
    "\n",
    "predictions = label_encoder.inverse_transform(classifier2.predict(X_test))\n",
    "\n",
    "CreateSubmission (test_ids, predictions, \"svc_rbf\", folder_path = device_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPFFPqqJpKpe"
   },
   "source": [
    "## (6)\n",
    "## Model: Neural Networks\n",
    "## Requirements: Tensorflow and Keras\n",
    "### Computationally Intensive!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5q1FW-afWwHG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers.core import Dense, Dropout\n",
    "from tensorflow.python.keras.callbacks import History \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iSbQmq4ixcIL"
   },
   "outputs": [],
   "source": [
    "def CreateNeuralNetwork (input_size, neurons_1, neurons_2, dropout = 0.50, initial_dropout = True):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    if initial_dropout == True:\n",
    "        model.add(Dropout(dropout, input_shape = (input_size, )))\n",
    "        model.add(Dense(neurons_1, activation = \"relu\", kernel_initializer = \"glorot_uniform\"))\n",
    "    \n",
    "    else:\n",
    "        model.add(Dense(neurons_1, activation = \"relu\", kernel_initializer = \"glorot_uniform\", input_shape = (input_size, )))    \n",
    "\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(neurons_2, activation = \"relu\", kernel_initializer = \"glorot_uniform\"))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(20, activation = \"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer = \"adadelta\", loss = \"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Change verbose to True in order to see epochs (fit() function call)\n",
    "def GetTrainedNeuralNetwork (X, y, iterations = 1, epochs = 2000, neurons = [256, 32], dropout = 0.50, batch_size = 2048, val_split = 0.2, initial_dropout = True):\n",
    "\n",
    "    training_history = History()\n",
    "\n",
    "    neural_network = CreateNeuralNetwork (X.shape[1], neurons[0], neurons[1], dropout, initial_dropout)\n",
    "    neural_network.fit(X, y, epochs = epochs, batch_size = batch_size, callbacks = [training_history], validation_split = val_split, verbose = False)\n",
    "    \n",
    "    return neural_network, training_history\n",
    "    \n",
    "def GetStatString (n1, n2, d, ind, e):\n",
    "    \n",
    "    return \"network_({},{})_dropout_({}_{})_epochs_{}\".format(str('%04d' % n1), str('%03d' % n2), str(ind), str(d), str(e)) \n",
    "    \n",
    "\n",
    "def WriteTrainingHistoryToFile (history, stats, folder_path = device_folder):\n",
    "    \n",
    "    file_name = folder_path + \"run_\" + stats + \".txt\"\n",
    "    \n",
    "    data = re.sub(\"'\", '\"', str(history.history))\n",
    "    \n",
    "    with open (file_name, \"w\") as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Score: 81.566%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "id": "RKKh751PmkuT",
    "outputId": "a37b80aa-fdc5-49a9-b8a3-1a58544a3584"
   },
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "epochs = 2000\n",
    "neurons = [1024, 128]\n",
    "dropout = 0.50\n",
    "predictions = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    neural_network, training_history = GetTrainedNeuralNetwork (X, y, epochs = epochs, neurons = neurons, dropout = dropout, val_split = 0)\n",
    "    predictions.append(neural_network.predict_proba(X_test))\n",
    "\n",
    "final_predictions = np.mean(predictions, axis=0)\n",
    "y_test = label_encoder.inverse_transform(np.argmax(final_predictions, axis=1))\n",
    "\n",
    "stats = GetStatString (neurons[0], neurons[1], dropout, True, epochs)\n",
    "\n",
    "\n",
    "CreateSubmission (test_ids, y_test, stats, folder_path = device_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pJlmqK2zK6U"
   },
   "source": [
    "## Second Best Score: 81.345%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PRqBMKZxJkG"
   },
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "epochs = 2500\n",
    "neurons = [512, 128]\n",
    "dropout = 0.50\n",
    "predictions = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    neural_network, training_history = GetTrainedNeuralNetwork (X, y, epochs = epochs, neurons = neurons, dropout = dropout, val_split = 0)\n",
    "    predictions.append(neural_network.predict_proba(X_test))\n",
    "\n",
    "final_predictions = np.mean(predictions, axis=0)\n",
    "y_test = label_encoder.inverse_transform(np.argmax(final_predictions, axis=1))\n",
    "\n",
    "\n",
    "stats = GetStatString (neurons[0], neurons[1], dropout, True, epochs)\n",
    "CreateSubmission (test_ids, y_test, stats, folder_path = device_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mG9d5dsdpKpy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final Submission.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
